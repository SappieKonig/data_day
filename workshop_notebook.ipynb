{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Workshop: Retrieval Augmented Generation (RAG)\n",
    "\n",
    "Welkom bij deze introductie workshop over RAG: Retrieval Augmented Generation.\n",
    "\n",
    "RAG is een techniek om Large Language Models (LLMs) te voorzien van externe kennis, zodat ze vragen kunnen beantwoorden over informatie die niet in hun oorspronkelijke trainingsdata zat.\n",
    "\n",
    "**Het proces bestaat uit 3 hoofdstappen:**\n",
    "1.  **Retrieval (ophalen):** Je stelt een vraag. Op basis van deze vraag worden de meest relevante stukjes informatie (documenten/tekstfragmenten) in een kennisdatabase (vaak een vector database) gezocht.\n",
    "2.  **Augmentation (verrijken):** De oorspronkelijke vraag wordt gecombineerd (verrijkt) met de gevonden informatie.\n",
    "3.  **Generation (genereren):** De verrijkte vraag wordt aan een LLM gegeven, die vervolgens een antwoord genereert op basis van *zowel* de oorspronkelijke vraag *als* de opgehaalde informatie.\n",
    "\n",
    "Heerlijk simpel in concept, en ontzettend krachtig als het goed werkt!\n",
    "Vandaag gaan we een duik nemen in RAG, en kijken hoe we dit kunnen toepassen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vereisten & Setup\n",
    "\n",
    "Voordat we beginnen, moeten we zorgen dat we de benodigde Python libraries geïnstalleerd hebben en onze OpenAI API key instellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installeer de benodigde packages\n",
    "# - openai: Voor interactie met OpenAI API's (embeddings, LLMs)\n",
    "# - langchain & co: Een framework om LLM applicaties te bouwen (incl. RAG)\n",
    "# - pypdf: Om PDF bestanden te lezen\n",
    "# - chromadb: Een populaire vector database\n",
    "# - tiktoken: Wordt gebruikt door Langchain/OpenAI voor token counting\n",
    "# - requests: Om bestanden van het web te downloaden\n",
    "# - gdown: Om bestanden van Google Drive te downloaden\n",
    "%pip install openai langchain langchain-openai langchain-chroma pypdf chromadb tiktoken requests gdown langchain-community -q > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importeer de benodigde modules\n",
    "import os\n",
    "import requests\n",
    "import zipfile\n",
    "import gdown\n",
    "from getpass import getpass\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OpenAI API Key Configuratie ---\n",
    "\n",
    "# Belangrijk: Deel je API key nooit publiekelijk.\n",
    "# Je kunt een OpenAI API key krijgen door een account aan te maken op:\n",
    "# https://platform.openai.com/api-keys\n",
    "\n",
    "# We proberen de key eerst uit de omgeving te lezen (veiliger).\n",
    "# Als dat niet lukt, vragen we erom via een beveiligd invoerveld.\n",
    "openai_api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "if not openai_api_key:\n",
    "    openai_api_key = getpass(\"Voer je OpenAI API Key in: \")\n",
    "\n",
    "# Stel de environment variable in voor deze sessie (vereist door Langchain)\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_api_key\n",
    "\n",
    "print(\"OpenAI API Key ingesteld.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 1: Het Bouwen van een Kennisdatabase\n",
    "\n",
    "Een RAG systeem heeft een database nodig waar het relevante informatie uit kan ophalen. Vaak is dit een **vector database**. We vullen deze database met onze eigen data.\n",
    "\n",
    "We gebruiken hier als voorbeeld een publiek beschikbaar regioplan uit het Integraal Zorgakkoord (IZA). Deze zijn te vinden op [de website van De Juiste Zorg op de Juiste Plek](https://www.dejuistezorgopdejuisteplek.nl/programmas/integraal-zorgakkoord/iza-onderdelen/regionale-samenwerking/regiobeelden-en-plannen/).\n",
    "\n",
    "Laten we beginnen met het downloaden en inladen van een specifiek regioplan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download een regioplan (voorbeeld: Noord-Holland Noord)\n",
    "pdf_url = \"https://www.dejuistezorgopdejuisteplek.nl/.wh/ea/uc/febe8c8ce01023a1ab6007074a602087b21454d15638300/Regioplan%20Noord-Holland%20Noord%20versie%201-1-2024.pdf\"\n",
    "pdf_filename = \"regioplan_nhn.pdf\"\n",
    "\n",
    "response = requests.get(pdf_url)\n",
    "with open(pdf_filename, 'wb') as f:\n",
    "    f.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad de tekst uit de gedownloade PDF\n",
    "loader = PyPDFLoader(pdf_filename)\n",
    "pages = loader.load()\n",
    "print(f\"PDF geladen: {len(pages)} pagina's\")\n",
    "print(f\"\\nVoorbeeld (eerste 500 tekens van pagina 42):\\n{pages[42].page_content[:500]}...\")\n",
    "print(f\"\\nMetadata: {pages[42].metadata}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings en Vector Databases: De Kern van RAG Retrieval\n",
    "\n",
    "Nu we de data hebben ingeladen, moeten we deze voorbereiden voor het RAG-proces. Hier komen **embeddings** en **vector databases** om de hoek kijken.\n",
    "\n",
    "**Embeddings:**\n",
    "Een *embedding* is een numerieke representatie (een lijst getallen, oftewel een *vector*) van een stukje tekst. Het bijzondere is dat deze vector de *semantische betekenis* van de tekst vastlegt. Teksten die qua betekenis op elkaar lijken, krijgen vectoren die wiskundig dicht bij elkaar liggen in een hoog-dimensionale 'vectorruimte'. Modellen zoals OpenAI's `text-embedding-3-small` zijn getraind om deze betekenisvolle vectoren te genereren.\n",
    "\n",
    "**Vector Databases:**\n",
    "Een *vector database* is speciaal ontworpen om deze vectoren efficiënt op te slaan en te doorzoeken op basis van gelijkenis. Wanneer je een vraag stelt, wordt die vraag óók omgezet in een vector. De database zoekt dan razendsnel naar de opgeslagen vectoren (die bij onze document-fragmenten horen) die het meest 'lijken' op de vector van de vraag. Dit 'lijken' wordt vaak gemeten met methoden zoals *cosine similarity*. Hoe dichter de vectoren bij elkaar liggen, hoe relevanter het bijbehorende tekstfragment waarschijnlijk is voor de vraag.\n",
    "\n",
    "**Splitsen (Chunking):**\n",
    "Voordat we embeddings maken, splitsen we de documenten op in kleinere, behapbare stukjes ('chunks'). Dit heeft meerdere redenen:\n",
    "1.  **Relevantie:** Kleinere chunks zorgen voor specifiekere resultaten bij het zoeken.\n",
    "2.  **Context Limieten:** LLMs hebben een limiet aan hoeveel tekst (context window) ze in één keer kunnen verwerken. Chunks passen hier beter binnen.\n",
    "3.  **Efficiëntie:** Het is efficiënter om embeddings voor kleinere stukjes te berekenen en op te slaan.\n",
    "\n",
    "We gebruiken hiervoor een `TextSplitter` die probeert slim te splitsen (bv. op paragrafen) en enige overlap tussen chunks behoudt om context niet te verliezen.\n",
    "\n",
    "**Metadata Toevoegen:**\n",
    "Voordat we de chunks opslaan in de vector database, kunnen we ze verrijken met metadata. Dit is extra informatie over de chunks die later nuttig kan zijn voor:\n",
    "1. **Filtering:** Zoeken binnen specifieke documenttypes of regio's\n",
    "2. **Context voor het LLM:** Informatie over de bron meegeven\n",
    "3. **Bronvermelding:** Traceren waar informatie vandaan komt\n",
    "\n",
    "Elke chunk heeft standaard al metadata zoals `source` (bestandsnaam) en `page` (paginanummer). We kunnen dit uitbreiden met bijvoorbeeld `document_type` en `region` om later gerichter te kunnen zoeken."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Een vector database bouwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiseer het OpenAI embedding model ('text-embedding-3-small', een modern en efficiënt model)\n",
    "embeddings_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "# Definieer waar ChromaDB de lokale vector database opslaat\n",
    "persist_directory_single = 'chroma_db_single_doc'\n",
    "print(f\"Vector database voor enkel document wordt opgeslagen in: {persist_directory_single}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splits document op in chunks met RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=200,\n",
    "    length_function=len, is_separator_regex=False\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages) if pages else []\n",
    "\n",
    "# Voeg eventueel extra metadata toe aan elke chunk\n",
    "for chunk in chunks:\n",
    "    chunk.metadata[\"document_type\"] = \"Regioplan IZA\"\n",
    "    chunk.metadata[\"regio\"] = \"Noord-Holland Noord\" \n",
    "\n",
    "print(f\"Document opgesplitst in {len(chunks)} chunks.\")\n",
    "print(\"\\n--- Voorbeeld chunk 16 (eerste 500 tekens): ---\")\n",
    "print(chunks[16].page_content[:500] + \"...\")\n",
    "print(f\"\\nMetadata chunk 16: {chunks[16].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voeg chunks toe aan vector database\n",
    "# LET OP: Dit kan even duren en kost OpenAI credits!\n",
    "# Chroma.from_documents doet het volgende:\n",
    "# 1. Berekent de embedding voor elke chunk met `embeddings_model`.\n",
    "# 2. Slaat de chunk tekst, metadata, én de embedding op in de database.\n",
    "# 3. Slaat de database op schijf op in `persist_directory_single`.\n",
    "\n",
    "assert chunks, \"Geen chunks beschikbaar, kan database niet maken.\"\n",
    "assert embeddings_model, \"Geen embedding model beschikbaar, kan database niet maken.\"\n",
    "\n",
    "vector_db_single = Chroma.from_documents(\n",
    "    documents=chunks,\n",
    "    embedding=embeddings_model, \n",
    "    persist_directory=persist_directory_single\n",
    ")\n",
    "\n",
    "print(f\"Vector database aangemaakt met {vector_db_single._collection.count()} entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We hebben nu succesvol één document verwerkt en opgeslagen in een lokale vector database (`vector_db_single`). Je kunt je voorstellen dat je dit proces herhaalt voor al je documenten.\n",
    "\n",
    "Omdat het verwerken van veel documenten tijdrovend kan zijn, is er voor deze workshop een vooraf gebouwde vector database beschikbaar met *alle* regiobeelden en -plannen. Deze gaan we nu proberen te downloaden en inladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download en unzip de voorbereide vector database\n",
    "db_file_id = \"11qu_GGtEOQibDaLIgYJMt77739_qUWyG\"\n",
    "db_zip_filename = \"prebuilt_chroma_db.zip\"\n",
    "db_persist_directory_full = \"chroma_db_regioplannen\"\n",
    "\n",
    "if not os.path.exists(db_persist_directory_full):\n",
    "    if not os.path.exists(db_zip_filename):\n",
    "        print(\"Downloading database...\")\n",
    "        url = f\"https://drive.google.com/uc?id={db_file_id}\"\n",
    "        gdown.download(url, db_zip_filename, quiet=False)\n",
    "    \n",
    "    with zipfile.ZipFile(db_zip_filename, 'r') as zip_ref:\n",
    "        zip_ref.extractall(\".\")\n",
    "    print(\"Database uitgepakt naar huidige map\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laad de volledige, voorbereide database in het geheugen\n",
    "vector_db_full = None\n",
    "if os.path.exists(db_persist_directory_full) and embeddings_model:\n",
    "    print(f\"Laden van voorbereide vector database uit {db_persist_directory_full}...\")\n",
    "    vector_db_full = Chroma(\n",
    "        persist_directory=db_persist_directory_full,\n",
    "        embedding_function=embeddings_model\n",
    "    )\n",
    "    count = vector_db_full._collection.count()\n",
    "    print(f\"Volledige database geladen met {count} entries.\")\n",
    "    if count == 0:\n",
    "        print(\"Waarschuwing: Database is leeg!\")\n",
    "else:\n",
    "    reason = \"geen embedding model\" if not embeddings_model else f\"map {db_persist_directory_full} niet gevonden\"\n",
    "    print(f\"Kan database niet laden: {reason}\")\n",
    "\n",
    "# Kies actieve database - gebruik volledige db als beschikbaar, anders single doc db\n",
    "active_vector_db = None\n",
    "if vector_db_full and vector_db_full._collection.count() > 0:\n",
    "    active_vector_db = vector_db_full\n",
    "    print(f\"Actieve database: Volledige database ('{db_persist_directory_full}')\")\n",
    "elif vector_db_single and vector_db_single._collection.count() > 0:\n",
    "    active_vector_db = vector_db_single\n",
    "    print(f\"Actieve database: Enkel document database ('{persist_directory_single}')\")\n",
    "else:\n",
    "    print(\"Geen actieve vector database beschikbaar.\")\n",
    "\n",
    "if active_vector_db:\n",
    "    print(f\"Actieve database bevat {active_vector_db._collection.count()} entries.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht: voeg een eigen document toe aan de vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check of het is gelukt, je kunt ook zonder dat dit is gelukt doorgaan.\n",
    "assert active_vector_db, \"Lees eerst de voorbereide vector database in.\"\n",
    "assert active_vector_db._collection.count() > 23320, \"In dit notebook kom je soms opdrachten tegen, leuk om eens te proberen voordat je verder gaat.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 2: Zoeken in de Vector Database (Retrieval)\n",
    "\n",
    "Nu we een actieve vector database hebben, kunnen we de kernfunctionaliteit gebruiken: het zoeken naar relevante informatie op basis van een vraag of zoekterm.\n",
    "\n",
    "Het proces is als volgt:\n",
    "1.  Neem de input vraag/zoekterm.\n",
    "2.  Genereer de embedding (vector) voor deze input met hetzelfde embedding model als waarmee de database is gemaakt (`text-embedding-3-small` in ons geval).\n",
    "3.  Gebruik de vector database om de `k` meest gelijkende vectoren (en dus de bijbehorende tekst chunks) te vinden.\n",
    "\n",
    "Laten we dit proberen met een voorbeeldvraag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Voorbeeld zoekopdracht\n",
    "query = \"Wat zijn de belangrijkste knelpunten in de ouderenzorg in Noord-Holland Noord?\"\n",
    "k_results = 4  # Aantal relevante chunks om op te halen\n",
    "\n",
    "assert active_vector_db, \"Geen actieve vector database om in te zoeken.\"\n",
    "\n",
    "print(f\"Zoeken naar {k_results} relevante chunks voor de vraag: '{query}'...\")\n",
    "retrieved_docs = active_vector_db.similarity_search(query, k=k_results)\n",
    "\n",
    "print(f\"\\n{len(retrieved_docs)} relevante documenten/chunks gevonden:\")\n",
    "if not retrieved_docs:\n",
    "    print(\">>> Geen resultaten gevonden. Probeer een andere vraag of controleer de database inhoud.\")\n",
    "    \n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Document {i+1} (Bron: {doc.metadata.get('source', 'Onbekend')}, \"\n",
    "          f\"Pagina: {doc.metadata.get('page', 'Onbekend')}) ---\")\n",
    "    content_snippet = doc.page_content[:500] + \"...\" if len(doc.page_content) > 500 else doc.page_content\n",
    "    print(content_snippet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoals je ziet, haalt de similarity search de tekstfragmenten op die qua betekenis het dichtst bij de vraag liggen. Dit is de *Retrieval* stap in RAG. De kwaliteit van deze stap is cruciaal voor het uiteindelijke antwoord."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht: zoek uit welke documenten er allemaal in de vector database zitten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht: zoek uit hoe je kunt filteren op alleen regels uit het 'Subregionale ROAZ-plan Noord-Holland Noord.pdf' document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stap 3: Genereren van een Antwoord (Generation)\n",
    "\n",
    "We hebben nu:\n",
    "1.  De oorspronkelijke vraag van de gebruiker.\n",
    "2.  Een set relevante document chunks uit onze database (`retrieved_docs`).\n",
    "\n",
    "De laatste stap is om deze informatie te combineren en aan een Large Language Model (LLM) te geven, met de instructie om de vraag te beantwoorden *op basis van de aangeleverde context*. Dit zijn de *Augmentation* en *Generation* stappen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialiseer LLM (gpt-4o) met lage temperatuur voor feitelijke antwoorden\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o\", temperature=0.1)\n",
    "print(\"LLM (gpt-4o) initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definieer de RAG chain met OpenAI direct\n",
    "\n",
    "# Bouw een simpele prompt template voor RAG\n",
    "rag_prompt_template = \"\"\"\\\n",
    "Je bent een behulpzame assistent die vragen beantwoordt op basis van de gegeven context.\n",
    "Gebruik alleen de onderstaande context om de vraag te beantwoorden.\n",
    "Als je het antwoord niet kunt vinden in de context, zeg dat dan eerlijk.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Vraag: {question}\n",
    "\n",
    "Antwoord:\"\"\"\n",
    "\n",
    "assert active_vector_db, \"Kan RAG niet uitvoeren: Vector database is niet beschikbaar.\"\n",
    "assert llm, \"Kan RAG niet uitvoeren: LLM is niet beschikbaar.\"\n",
    "\n",
    "print(f\"\\nAntwoord genereren voor de vraag: '{query}'...\")\n",
    "\n",
    "# 1. Haal relevante documenten op met similarity search\n",
    "# (Optioneel) filter op metadata\n",
    "retrieved_docs = active_vector_db.similarity_search(\n",
    "    query,\n",
    "    k=k_results,\n",
    ")\n",
    "\n",
    "# 2. Combineer de documenten tot één context string\n",
    "context = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "\n",
    "# 3. Vul de prompt template in\n",
    "prompt = rag_prompt_template.format(\n",
    "    context=context,\n",
    "    question=query\n",
    ")\n",
    "\n",
    "# 4. Stuur de prompt naar het LLM en krijg een antwoord\n",
    "response = llm.invoke(prompt)\n",
    "\n",
    "print(\"\\n--- Gegenereerd Antwoord: ---\")\n",
    "print(response.content)\n",
    "\n",
    "# Voorbeeld van een andere vraag:\n",
    "# print(\"\\nProbeer een andere vraag:\")\n",
    "# query_2 = \"Wat zijn de plannen voor digitale zorg?\"\n",
    "# print(f\"\\nAntwoord genereren voor de vraag: '{query_2}'...\")\n",
    "# retrieved_docs_2 = active_vector_db.similarity_search(query_2, k=k_results)\n",
    "# context_2 = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs_2)\n",
    "# prompt_2 = rag_prompt_template.format(context=context_2, question=query_2)\n",
    "# response_2 = llm.invoke(prompt_2)\n",
    "# print(\"\\n--- Gegenereerd Antwoord (Vraag 2): ---\")\n",
    "# print(response_2.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Opdracht: probeer verschillen te vinden tussen hoe twee regioplannen een zelfde vraag beantwoorden. \n",
    "\n",
    "Bijv. hoe verschillen de knelpunten in de ouderenzorg tussen Noord-Holland Noord en Flevoland? \n",
    "\n",
    "*Tip: gebruik hiervoor de bestandnamen die je bij de vorige opdrachten hebt gevonden.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jouw code hier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusie & Volgende Stappen\n",
    "\n",
    "Gefeliciteerd! Je hebt zojuist een volledig RAG-systeem gebouwd en uitgevoerd:\n",
    "1.  Een document gedownload en verwerkt.\n",
    "2.  Embeddings gegenereerd en opgeslagen in een vector database (ChromaDB).\n",
    "3.  Geprobeerd een grotere, voorbereide database te laden.\n",
    "4.  Relevante documenten opgehaald (Retrieval) op basis van een vraag.\n",
    "5.  De vraag en context aan een LLM gegeven om een antwoord te genereren (Generation).\n",
    "\n",
    "**Mogelijke verbeteringen en experimenten om je resultaten NOG beter te maken:**\n",
    "*   **Retriever instellingen:** Pas het aantal opgehaalde documenten (`k`) aan. Experimenteer met andere retrieval methoden (bv. `similarity_score_threshold` of `MultiQueryRetriever`).\n",
    "*   **Metadata gebruiken:** Probeer de retrieval stap een handje te helpen door metadata toe te voegen aan je query door een specifiek document te specificeren.\n"
    "*   **Andere LLMs:** Experimenteer met o3-mini, GPT-4.1, of andere modellen.\n",
    "*   **Prompt Engineering:** Pas de `rag_prompt_template` aan voor betere resultaten of een andere 'tone of voice'.\n",
    "\n",
    "RAG is een krachtige techniek met veel toepassingsmogelijkheden. Hopelijk heeft deze workshop je een goed startpunt gegeven!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
